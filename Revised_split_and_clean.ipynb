{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMZBGAeabpm3vjQrfbQs7le",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LanceTBag/Data-Clean-and-Split---November/blob/main/Revised_split_and_clean.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import os\n",
        "import logging\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "def extract(input_file, separator):\n",
        "    try:\n",
        "        # Read the CSV file with the user-specified separator\n",
        "        df = pd.read_csv(input_file, encoding='utf-8', sep=separator)\n",
        "        logging.info(f\"Successfully loaded {input_file} with separator '{separator}'\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error reading {input_file}: {e}\")\n",
        "        raise e\n",
        "\n",
        "def transform(df, input_file):\n",
        "    # Initialize invalid rows dataframe and issue column\n",
        "    invalid_rows = pd.DataFrame(columns=df.columns.tolist() + ['Issue'])\n",
        "\n",
        "    # Clean invalid characters\n",
        "    def clean_text(text):\n",
        "        if isinstance(text, str):\n",
        "            return re.sub(r'â€|[^\\x00-\\x7F]+', '', text)  # Removes non-ASCII characters\n",
        "        return text\n",
        "\n",
        "    # Apply cleaning functions\n",
        "    df_clean = df.copy()\n",
        "\n",
        "    # Step 1: Clean text for all columns\n",
        "    for column in df_clean.columns:\n",
        "        df_clean[column] = df_clean[column].map(clean_text)\n",
        "\n",
        "    # Step 2: Handle missing values (remove records where login_id or mail_address is missing)\n",
        "    missing_login_id_rows = df_clean[df_clean['login_id'].isnull()]\n",
        "    if not missing_login_id_rows.empty:\n",
        "        missing_login_id_rows['Issue'] = 'Missing login_id'\n",
        "        invalid_rows = pd.concat([invalid_rows, missing_login_id_rows], ignore_index=True)\n",
        "\n",
        "    missing_mail_address_rows = df_clean[df_clean['mail_address'].isnull()]\n",
        "    if not missing_mail_address_rows.empty:\n",
        "        missing_mail_address_rows['Issue'] = 'Missing mail_address'\n",
        "        invalid_rows = pd.concat([invalid_rows, missing_mail_address_rows], ignore_index=True)\n",
        "\n",
        "    df_clean = df_clean.dropna(subset=['login_id', 'mail_address'])\n",
        "\n",
        "    # Step 3: Handle duplicates by keeping one and removing the others\n",
        "    duplicate_rows = df_clean[df_clean.duplicated(subset=['login_id', 'mail_address'], keep='first')]\n",
        "    if not duplicate_rows.empty:\n",
        "        duplicate_rows['Issue'] = 'Duplicate'\n",
        "        invalid_rows = pd.concat([invalid_rows, duplicate_rows], ignore_index=True)\n",
        "    df_clean = df_clean.drop_duplicates(subset=['login_id', 'mail_address'], keep='first')\n",
        "\n",
        "    # Update the garbage file name based on input file\n",
        "    base_filename = os.path.splitext(os.path.basename(input_file))[0]\n",
        "    garbage_file = f\"{base_filename}_garbage.csv\"\n",
        "\n",
        "    return df_clean, invalid_rows, garbage_file\n",
        "\n",
        "def load(df_cleaned, output_file, invalid_rows, garbage_file):\n",
        "    try:\n",
        "        # Save the cleaned DataFrame to a new CSV file\n",
        "        df_cleaned.to_csv(output_file, index=False, encoding='utf-8', sep=',')\n",
        "        logging.info(f\"Cleaned data written to {output_file}\")\n",
        "\n",
        "        # Save the invalid/garbage records to a separate CSV file\n",
        "        invalid_rows.to_csv(garbage_file, index=False, encoding='utf-8', sep=',')\n",
        "        logging.info(f\"Invalid records written to {garbage_file}\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error writing to file: {e}\")\n",
        "        raise e\n",
        "\n",
        "def split_file(output_file, chunks):\n",
        "    # Read the cleaned output file\n",
        "    df = pd.read_csv(output_file, encoding='utf-8')\n",
        "    rows_per_chunk = len(df) // chunks\n",
        "    base_filename = os.path.splitext(output_file)[0]\n",
        "\n",
        "    # Split the file into chunks\n",
        "    for i in range(chunks):\n",
        "        start = i * rows_per_chunk\n",
        "        end = None if i == chunks - 1 else (i + 1) * rows_per_chunk\n",
        "        chunk = df.iloc[start:end]\n",
        "        split_file_name = f\"{base_filename}_{i + 1}.csv\"\n",
        "        chunk.to_csv(split_file_name, index=False, encoding='utf-8', sep=',')\n",
        "        logging.info(f\"Split file {split_file_name} created with {len(chunk)} records\")\n",
        "\n",
        "def modify_created_at(df):\n",
        "    if 'created_at' in df.columns:\n",
        "        # Modify the created_at column to only keep the 'yyyy-mm-dd' format\n",
        "        df['created_at'] = pd.to_datetime(df['created_at'], errors='coerce').dt.date\n",
        "    return df\n",
        "\n",
        "def main():\n",
        "    # Prompt for input file, separator, and chunk number\n",
        "    input_file = input(\"Enter the input CSV file name: \")\n",
        "    separator = input(\"Enter the separator used in the input CSV file: \")\n",
        "    chunks = int(input(\"Enter the number of chunks to split the file into: \"))\n",
        "\n",
        "    # Generate filenames based on the input file\n",
        "    base_filename = os.path.splitext(os.path.basename(input_file))[0]\n",
        "    output_file = f\"{base_filename}_nonduplicate.csv\"\n",
        "\n",
        "    # Step 1: Extract\n",
        "    df = extract(input_file, separator)\n",
        "\n",
        "    # Step 2: Transform\n",
        "    df_cleaned, invalid_rows, garbage_file = transform(df, input_file)\n",
        "\n",
        "    # Modify 'created_at' column in the cleaned DataFrame\n",
        "    df_cleaned = modify_created_at(df_cleaned)\n",
        "\n",
        "    # Step 3: Load cleaned and invalid data to respective files\n",
        "    load(df_cleaned, output_file, invalid_rows, garbage_file)\n",
        "\n",
        "    # Step 4: Split cleaned file into chunks\n",
        "    split_file(output_file, chunks)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zq8zEGOe7O99",
        "outputId": "0374a0b3-4e2c-4c54-bcac-6d74e8c22d84"
      },
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the input CSV file name: /content/Japan-2019.csv\n",
            "Enter the separator used in the input CSV file: ;\n",
            "Enter the number of chunks to split the file into: 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-e090c16dbd39>:12: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(input_file, encoding='utf-8', sep=separator)\n",
            "<ipython-input-2-e090c16dbd39>:39: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  missing_login_id_rows['Issue'] = 'Missing login_id'\n",
            "<ipython-input-2-e090c16dbd39>:40: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  invalid_rows = pd.concat([invalid_rows, missing_login_id_rows], ignore_index=True)\n",
            "<ipython-input-2-e090c16dbd39>:52: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  duplicate_rows['Issue'] = 'Duplicate'\n",
            "<ipython-input-2-e090c16dbd39>:77: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(output_file, encoding='utf-8')\n"
          ]
        }
      ]
    }
  ]
}